# LLM_Evaluation_-_QA_Toolkit
LLM Evaluation and QA Toolkit is a Python-based framework to test and evaluate Large Language Models using predefined prompts. It supports exact match, similarity scoring, and toxicity checks, automating quality assurance and generating performance reports for LLM outputs.
